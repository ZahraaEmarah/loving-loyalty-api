# -*- coding: utf-8 -*-
"""inventory-management

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/nse-gcp-ema-tt-0eee5-sbx-1/locations/europe-west1/repositories/d25d681b-251a-44de-9176-03c46687a15f
"""

from google.cloud import bigquery
from google.auth.exceptions import DefaultCredentialsError

try:
    client = bigquery.Client.from_service_account_json("service-account.json")
except (FileNotFoundError, DefaultCredentialsError):
    client = bigquery.Client()

# places_query = "SELECT * FROM LovingLoyaltyDB.dim_places"
# places_df = client.query(places_query).to_dataframe()

# o_query = "SELECT * FROM LovingLoyaltyDB.fct_orders"
# o_df = client.query(o_query).to_dataframe()

# oi_query = "SELECT * FROM LovingLoyaltyDB.fct_orders_items"
# oi_df = client.query(oi_query).to_dataframe()

# """## Remove unwanted fields from the 3 tables"""

# #remove unwanted fields
# # prompt: have df_places only the columns id, title, latitue, longitude

# places_df = places_df[['id', 'title', 'latitude', 'longitude']]
# o_df = o_df[['id', 'created', 'place_id', 'status']]
# oi_df = oi_df[['id', 'created', 'title', 'item_id', 'order_id', 'status']]

# o_df['status'].unique()

# oi_df['status'].unique()

import pandas as pd

# # Step 1: Filter relevant data
# oi_filtered_df = oi_df[oi_df['status'] == 'Fulfilled']

# # Step 2: Convert epoch time to datetime
# o_df['created'] = pd.to_datetime(o_df['created'], unit='s')
# oi_df['created'] = pd.to_datetime(oi_df['created'], unit='s')

# # Step 3: Join DataFrames
# orders_with_items = pd.merge(
#     o_df, oi_df, left_on='id', right_on='order_id', suffixes=('_order', '_item')
# )
# enriched_data = pd.merge(
#     orders_with_items, places_df, left_on='place_id', right_on='id', suffixes=('', '_place')
# )

# # Step 4: Extract week
# enriched_data['week'] = enriched_data['created_order'].dt.to_period('W')

# # Step 5: Aggregate data
# aggregated_data = (
#     enriched_data.groupby(['place_id', 'item_id', 'week'])
#     .size()
#     .reset_index(name='total_orders')
# )

# # Step 6: Add week_starting column
# aggregated_data['week_starting'] = aggregated_data['week'].apply(lambda x: x.start_time)

# aggregated_data

# #aggregated_data = aggregated_data[aggregated_data['week_starting'] > "2022-05-01 00:00:00"]

# # prompt: show me distribution of total_order in aggregated_data

import matplotlib.pyplot as plt
import seaborn as sns

# # Distribution of total_orders
# plt.figure(figsize=(10, 6))
# sns.histplot(aggregated_data['total_orders'], kde=True)
# plt.title('Distribution of Total Orders')
# plt.xlabel('Total Orders')
# plt.ylabel('Frequency')
# plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# # Step 1: Pivot the data
# pivoted_data = aggregated_data.pivot_table(
#     index='week_starting', columns=['place_id', 'item_id'], values='total_orders', fill_value=0
# )

# # Step 2: Ensure all months are represented
# pivoted_data.index = pd.to_datetime(pivoted_data.index.astype(str))  # Convert month to datetime

# # Step 3: Visualize the time series for a specific place_id and item_id
# place_id = 64276
# item_id = 64299

# if (place_id, item_id) in pivoted_data.columns:
#     pivoted_data[(place_id, item_id)].plot(figsize=(10, 6), title=f"Time Series for Place {place_id}, Item {item_id}")
#     plt.xlabel("Week")
#     plt.ylabel("Total Orders")
#     plt.grid()
#     plt.show()
# else:
#     print(f"No data available for Place {place_id} and Item {item_id}")

# # Step 4: Decompose the time series
# if (place_id, item_id) in pivoted_data.columns:
#     ts = pivoted_data[(place_id, item_id)]
#     decomposition = seasonal_decompose(ts, model='additive', period=12)  # Assuming monthly data
#     decomposition.plot()
#     plt.show()
# else:
#     print(f"No data available for decomposition for Place {place_id} and Item {item_id}")

# # Step 5: Forecasting (Optional)
# # You can use ARIMA, SARIMA, or Prophet for forecasting.
# # Example: Using ARIMA (requires statsmodels library)
from statsmodels.tsa.arima.model import ARIMA

# if (place_id, item_id) in pivoted_data.columns:
#     ts = pivoted_data[(place_id, item_id)]
#     model = ARIMA(ts, order=(1, 1, 1))  # Adjust ARIMA parameters as needed
#     model_fit = model.fit()
#     forecast = model_fit.forecast(steps=30)  # Forecast the next 12 months
#     print(forecast)
# else:
#     print(f"No data available for forecasting for Place {place_id} and Item {item_id}")

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# # Step 1: Pivot the data
# pivoted_data = aggregated_data.pivot_table(
#     index='week_starting', columns=['place_id', 'item_id'], values='total_orders', fill_value=0
# )

# # Step 2: Ensure all weeks are represented
# pivoted_data.index = pd.to_datetime(pivoted_data.index)  # Ensure index is datetime for weekly data

# # Step 3: Visualize the time series for a specific place_id and item_id
# place_id = 94025
# item_id = 94551

# if (place_id, item_id) in pivoted_data.columns:
#     pivoted_data[(place_id, item_id)].plot(
#         figsize=(10, 6),
#         title=f"Time Series for Place {place_id}, Item {item_id}"
#     )
#     plt.xlabel("Week Starting")
#     plt.ylabel("Total Orders")
#     plt.grid()
#     plt.show()
# else:
#     print(f"No data available for Place {place_id} and Item {item_id}")

# # Step 4: Decompose the time series
# if (place_id, item_id) in pivoted_data.columns:
#     ts = pivoted_data[(place_id, item_id)]
#     decomposition = seasonal_decompose(ts, model='additive', period=52)  # Weekly data, 52 weeks in a year
#     decomposition.plot()
#     plt.show()
# else:
#     print(f"No data available for decomposition for Place {place_id} and Item {item_id}")

# # Step 5: Forecasting (Optional)
# # Example: Using ARIMA (requires statsmodels library)
from statsmodels.tsa.arima.model import ARIMA

# if (place_id, item_id) in pivoted_data.columns:
#     ts = pivoted_data[(place_id, item_id)]
#     model = ARIMA(ts, order=(2, 1, 2))  # Adjust ARIMA parameters as needed
#     model_fit = model.fit()
#     forecast = model_fit.forecast(steps=20)  # Forecast the next 12 weeks
#     print("Forecast for the next 12 weeks:")
#     print(forecast)
#     # Plot the forecast
#     plt.figure(figsize=(10, 6))
#     plt.plot(ts, label="Historical Data")
#     plt.plot(forecast.index, forecast, label="Forecast", color="red")
#     plt.title(f"Forecast for Place {place_id}, Item {item_id}")
#     plt.xlabel("Week Starting")
#     plt.ylabel("Total Orders")
#     plt.legend()
#     plt.grid()
#     plt.show()
# else:
#     print(f"No data available for forecasting for Place {place_id} and Item {item_id}")

def get_forecast_for_place_item(place_id, item_id, steps=12):
    import pandas as pd
    from statsmodels.tsa.arima.model import ARIMA

    # Use the previously computed aggregated_data and pivoted_data
    # If not already computed, re-compute here (for API use)
    global aggregated_data
    global pivoted_data

    try:
        # If not already defined, compute aggregated_data and pivoted_data
        if 'aggregated_data' not in globals() or 'pivoted_data' not in globals():
            # ...existing code to compute aggregated_data and pivoted_data...
            places_query = "SELECT * FROM LovingLoyaltyDB.dim_places"
            places_df = client.query(places_query).to_dataframe()
            o_query = "SELECT * FROM LovingLoyaltyDB.fct_orders"
            o_df = client.query(o_query).to_dataframe()
            oi_query = "SELECT * FROM LovingLoyaltyDB.fct_orders_items"
            oi_df = client.query(oi_query).to_dataframe()
            places_df = places_df[['id', 'title', 'latitude', 'longitude']]
            o_df = o_df[['id', 'created', 'place_id', 'status']]
            oi_df = oi_df[['id', 'created', 'title', 'item_id', 'order_id', 'status']]
            import pandas as pd
            oi_filtered_df = oi_df[oi_df['status'] == 'Fulfilled']
            o_df['created'] = pd.to_datetime(o_df['created'], unit='s')
            oi_df['created'] = pd.to_datetime(oi_df['created'], unit='s')
            orders_with_items = pd.merge(
                o_df, oi_df, left_on='id', right_on='order_id', suffixes=('_order', '_item')
            )
            enriched_data = pd.merge(
                orders_with_items, places_df, left_on='place_id', right_on='id', suffixes=('', '_place')
            )
            enriched_data['week'] = enriched_data['created_order'].dt.to_period('W')
            aggregated_data = (
                enriched_data.groupby(['place_id', 'item_id', 'week'])
                .size()
                .reset_index(name='total_orders')
            )
            aggregated_data['week_starting'] = aggregated_data['week'].apply(lambda x: x.start_time)
            pivoted_data = aggregated_data.pivot_table(
                index='week_starting', columns=['place_id', 'item_id'], values='total_orders', fill_value=0
            )
            pivoted_data.index = pd.to_datetime(pivoted_data.index)

        if (place_id, item_id) not in pivoted_data.columns:
            return None

        ts = pivoted_data[(place_id, item_id)]
        model = ARIMA(ts, order=(2, 1, 2))
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=steps)
        return forecast
    except Exception as e:
        print(f"Error in forecasting: {e}")
        return None









